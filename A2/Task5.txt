I trained 5 alignment models to use for decoding (on 1k, 10k, 15k, 30k, 100k sentences respectively), and used decoder2 on each of these together with the MLE language model. The results I got (using our ad-hoc scoring metric) were proprtions of 0.164, 0.195, 0.191, 0.195, 0.191 respectively. So other than the model trained with only 1k sentences, there doesn't appear to be any significant difference (at least by our metric)
As far as the actual evaluation metric, I feel our evaluation metric is not a good reflection of the quality of the translation. For one thing, the fact that we are strict in terms of the alignment of words (we only match words that are in exactly the same position) makes so that if there is the slightest offset, for example, an extra word followed by the exact right translation, then everything gets messed up and the score would be terrible despite the accurate translation.
Another issue is what we consider to be a good translation. For example, if our translation gets the structure of the sentence right, but the actual content horribly wrong (for instance, a couple of key verbs and nouns in the sentence are wrong, but the rest is correct), then we probably shouldn't be giving this a high score, but our metric does. Similarly, a sentence that is almost nonsensical but converys the right idea would get a really bad score.  
