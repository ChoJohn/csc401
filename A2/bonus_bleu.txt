For my bonus, I decided to implement the BLEU scoring metric in order to try to better analyze the results of our translations. The scoring algorithm itself can be found in score_bleu.m, and the script I ran to evaluate my already trained models using the BLEU scoring metric can be found in evalAlign_bleu.m. 
Looking at the actual results, I found that for models trained on 1k, 10k, 15k, 30k, 100k setences I saw BLEU scores of 0.0613, 0.0663, 0.0671, 0.0683, 0.0688 respectively on the test data. Contrasting this to the proportions I got in part 5, which were 0.164, 0.195, 0.191, 0.195, 0.191 respectively, at first glance we immediately see that this scoring metric does a better job of differentiating our models. In particular, momdels trained with more data have higher BLEU scores, and the difference seem at least somewhat meaningful.
Something worth noting (and you can see this in the script), is that I had to run BLEU scoring with N=2 (that is, the largest n-grams considered are of size 2). The standard for this would usually be 4, but because of how poor our translation algorithm is, there were no matches for any n-grams of size 3 or 4, which made it so that every model always received a BLEU score of 0. That is, 2 was the largest N such that the BLEU scores were somewhat informative.
